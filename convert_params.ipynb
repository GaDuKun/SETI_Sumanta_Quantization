{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a508b529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " quant_conv2d (quant_conv2D)    (None, 32, 32, 16)   448         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 16)  64          ['quant_conv2d[0][0]']           \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " quant_activation (quant_activa  (None, 32, 32, 16)  0           ['batch_normalization[0][0]']    \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 16)   0           ['quant_activation[0][0]']       \n",
      "                                                                                                  \n",
      " quant_conv2d_1 (quant_conv2D)  (None, 32, 32, 16)   2320        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 16)  64          ['quant_conv2d_1[0][0]']         \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " quant_activation_1 (quant_acti  (None, 32, 32, 16)  0           ['batch_normalization_1[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 16)   0           ['quant_activation_1[0][0]']     \n",
      "                                                                                                  \n",
      " quant_conv2d_2 (quant_conv2D)  (None, 32, 32, 16)   2320        ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 16)  64          ['quant_conv2d_2[0][0]']         \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 32, 32, 16)   0           ['activation[0][0]',             \n",
      "                                                                  'batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " quant_activation_2 (quant_acti  (None, 32, 32, 16)  0           ['add[0][0]']                    \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 32, 32, 16)   0           ['quant_activation_2[0][0]']     \n",
      "                                                                                                  \n",
      " quant_conv2d_3 (quant_conv2D)  (None, 16, 16, 32)   4640        ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 16, 16, 32)  128         ['quant_conv2d_3[0][0]']         \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " quant_activation_3 (quant_acti  (None, 16, 16, 32)  0           ['batch_normalization_3[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 16, 16, 32)   0           ['quant_activation_3[0][0]']     \n",
      "                                                                                                  \n",
      " quant_conv2d_4 (quant_conv2D)  (None, 16, 16, 32)   9248        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " quant_conv2d_5 (quant_conv2D)  (None, 16, 16, 32)   544         ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 16, 16, 32)  128         ['quant_conv2d_4[0][0]']         \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 16, 16, 32)   0           ['quant_conv2d_5[0][0]',         \n",
      "                                                                  'batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " quant_activation_4 (quant_acti  (None, 16, 16, 32)  0           ['add_1[0][0]']                  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 16, 16, 32)   0           ['quant_activation_4[0][0]']     \n",
      "                                                                                                  \n",
      " quant_conv2d_6 (quant_conv2D)  (None, 8, 8, 64)     18496       ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 8, 8, 64)    256         ['quant_conv2d_6[0][0]']         \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " quant_activation_5 (quant_acti  (None, 8, 8, 64)    0           ['batch_normalization_5[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 8, 8, 64)     0           ['quant_activation_5[0][0]']     \n",
      "                                                                                                  \n",
      " quant_conv2d_7 (quant_conv2D)  (None, 8, 8, 64)     36928       ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " quant_conv2d_8 (quant_conv2D)  (None, 8, 8, 64)     2112        ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 8, 8, 64)    256         ['quant_conv2d_7[0][0]']         \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 8, 8, 64)     0           ['quant_conv2d_8[0][0]',         \n",
      "                                                                  'batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " quant_activation_6 (quant_acti  (None, 8, 8, 64)    0           ['add_2[0][0]']                  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 8, 8, 64)     0           ['quant_activation_6[0][0]']     \n",
      "                                                                                                  \n",
      " quant_conv2d_9 (quant_conv2D)  (None, 4, 4, 128)    73856       ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 4, 4, 128)   512         ['quant_conv2d_9[0][0]']         \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " quant_activation_7 (quant_acti  (None, 4, 4, 128)   0           ['batch_normalization_7[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 4, 4, 128)    0           ['quant_activation_7[0][0]']     \n",
      "                                                                                                  \n",
      " quant_conv2d_10 (quant_conv2D)  (None, 4, 4, 128)   147584      ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " quant_conv2d_11 (quant_conv2D)  (None, 4, 4, 128)   8320        ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 4, 4, 128)   512         ['quant_conv2d_10[0][0]']        \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 4, 4, 128)    0           ['quant_conv2d_11[0][0]',        \n",
      "                                                                  'batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " quant_activation_8 (quant_acti  (None, 4, 4, 128)   0           ['add_3[0][0]']                  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 4, 4, 128)    0           ['quant_activation_8[0][0]']     \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 1, 1, 128)   0           ['activation_8[0][0]']           \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 128)          0           ['average_pooling2d[0][0]']      \n",
      "                                                                                                  \n",
      " quant_dense (quant_dense)      (None, 10)           1290        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 10)           0           ['quant_dense[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 310,090\n",
      "Trainable params: 309,098\n",
      "Non-trainable params: 992\n",
      "__________________________________________________________________________________________________\n",
      "Finish writing file !!!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import keras\n",
    "#from qkeras.utils import fold_batch_norm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras_model_quant_8bits import quant_conv2D,quant_activation, quant_dense\n",
    "custom_objects = {'quant_conv2D':  quant_conv2D, 'quant_activation':quant_activation, 'quant_dense':quant_dense}\n",
    "\n",
    "model = load_model('trained_models/trainedResnet_quant_8bits.h5', custom_objects = custom_objects)\n",
    "#model = fold_batch_norm(model)\n",
    "model.summary()\n",
    "\n",
    "file = open(\"binary_file.bin\", \"wb\")\n",
    "#file.write(\"#This file contain parameter of model\\n\")\n",
    "\n",
    "N_QuantBits = 8;\n",
    "epsilon = 1e-6;\n",
    "\n",
    "# Quantization function\n",
    "def quantize(W):\n",
    "  # Forward computation\n",
    "  quantized_range = 2**(N_QuantBits-1)\n",
    "  f = tf.round(W*quantized_range)/quantized_range\n",
    "  f = int(quantized_range*(f))\n",
    "  return f\n",
    "\n",
    "def batch_fold(mu,var,gamma, beta, epsilon, weights, biases):\n",
    "    \n",
    "    param1 = gamma*math.sqrt(var*var+epsilon)\n",
    "    param2 = -param1*mu + beta\n",
    "    \n",
    "    weights_new = weights*param1\n",
    "    biases_new = biases*param1 + param2\n",
    "    return weights_new,biases_new\n",
    "\n",
    "class my_conv:\n",
    "    def __init__(self, var1, var2):\n",
    "        self.w = var1\n",
    "        self.b = var2\n",
    "    def set_params(self, var1, var2):\n",
    "        self.w = var1\n",
    "        self.b = var2\n",
    "    def batch_fold(self,mu,var,gamma, beta, epsilon):\n",
    "    \n",
    "        param1 = gamma*np.sqrt(var*var+epsilon)\n",
    "        param2 = -param1*mu + beta\n",
    "        p = self.w\n",
    "        weights_new = self.w*param1\n",
    "        biases_new = self.b*param1 + param2\n",
    "        self.set_params(weights_new,biases_new)\n",
    "        #return weights_new,biases_new\n",
    "    def delete_params(self):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "quan_conv1 = my_conv(None, None)   # Using for batch_normalization folding\n",
    "quan_conv2 = my_conv(None, None)\n",
    "previous_layer = None\n",
    "#print(quan_conv1.w, quan_conv1.b)\n",
    "\n",
    "for layer in model.layers: \n",
    "    #print(layer.name)\n",
    "    #print(previous_layer)\n",
    "    \n",
    "    # Save parameters for batch normalization\n",
    "    if isinstance(layer, keras.layers.BatchNormalization):\n",
    "        \n",
    "       # Get the gamma, beta, moving_mean, and moving_variance parameters of the layer\n",
    "        gamma, beta, moving_mean, moving_variance = layer.get_weights()\n",
    "        #print(\"batch norm\",gamma.shape)\n",
    "       # Batch folding\n",
    "        quan_conv1.batch_fold(moving_mean,moving_variance,gamma, beta,epsilon)\n",
    "        \n",
    "        if (quan_conv2.w is None) and (quan_conv2.b is None): \n",
    "            # apply quantization for all elements and convert to integer\n",
    "            vec_quantize = np.vectorize(quantize)    # function for array input\n",
    "            kernel_arr =  vec_quantize(quan_conv1.w)\n",
    "            kernel_arr =  kernel_arr.astype(\"int\"+\"{}\".format(N_QuantBits))\n",
    "            \n",
    "            bias_arr = vec_quantize(quan_conv1.b)\n",
    "            bias_arr =  bias_arr.astype(\"int\"+\"{}\".format(N_QuantBits))\n",
    "            \n",
    "            # Convert to binary\n",
    "            vec_bin = np.vectorize(lambda x: np.binary_repr(x, width=N_QuantBits))\n",
    "            kernel_bin = vec_bin(kernel_arr)\n",
    "            bias_bin = vec_bin(bias_arr)\n",
    "            # Save to file\n",
    "            kernel_bin.tofile(file)\n",
    "            bias_bin.tofile(file)\n",
    "        else:\n",
    "            # Save simultaneous 2 quant_conv to binary file\n",
    "            \n",
    "            # The first one\n",
    "            # apply quantization for all elements and convert to integer\n",
    "            vec_quantize = np.vectorize(quantize)    # function for array input\n",
    "            kernel_arr =  vec_quantize(quan_conv1.w)\n",
    "            kernel_arr =  kernel_arr.astype(\"int\"+\"{}\".format(N_QuantBits))\n",
    "            \n",
    "            bias_arr = vec_quantize(quan_conv1.b)\n",
    "            bias_arr =  bias_arr.astype(\"int\"+\"{}\".format(N_QuantBits))\n",
    "            \n",
    "            # Convert to binary\n",
    "            vec_bin = np.vectorize(lambda x: np.binary_repr(x, width=N_QuantBits))\n",
    "            kernel_bin = vec_bin(kernel_arr)\n",
    "            bias_bin = vec_bin(bias_arr)\n",
    "            # Save to file\n",
    "            kernel_bin.tofile(file)\n",
    "            bias_bin.tofile(file)\n",
    "            \n",
    "            # The second one\n",
    "            # apply quantization for all elements and convert to integer\n",
    "            vec_quantize = np.vectorize(quantize)    # function for array input\n",
    "            kernel_arr =  vec_quantize(quan_conv2.w)\n",
    "            kernel_arr =  kernel_arr.astype(\"int\"+\"{}\".format(N_QuantBits))\n",
    "            \n",
    "            bias_arr = vec_quantize(quan_conv2.b)\n",
    "            bias_arr =  bias_arr.astype(\"int\"+\"{}\".format(N_QuantBits))\n",
    "            \n",
    "            # Convert to binary\n",
    "            vec_bin = np.vectorize(lambda x: np.binary_repr(x, width=N_QuantBits))\n",
    "            kernel_bin = vec_bin(kernel_arr)\n",
    "            bias_bin = vec_bin(bias_arr)\n",
    "            # Save to file\n",
    "            kernel_bin.tofile(file)\n",
    "            bias_bin.tofile(file)\n",
    "        # Delete parameters of 2 object quan_conv1 and quan_conv2 (for saving new parameters)\n",
    "        quan_conv1.delete_params()\n",
    "        quan_conv2.delete_params()\n",
    "        continue\n",
    "    #Save parameters for quant_conv2D\n",
    "    if 'quant_conv2d' in layer.name:    \n",
    "        \n",
    "        kernel, bias = layer.get_weights() \n",
    "        # reshape kernel and bias to (,num_numfilters)\n",
    "        kernel_arr = kernel.reshape((-1,kernel.shape[-1]))\n",
    "        bias_arr = bias\n",
    "        #print(kernel_arr.shape)\n",
    "        \n",
    "        # Check if there is exist 2 continuous layer\n",
    "        if quan_conv1.w is None and quan_conv1.b is None:\n",
    "            quan_conv1.set_params(kernel_arr,bias_arr)\n",
    "        else:\n",
    "            quan_conv2.set_params(kernel_arr,bias_arr)\n",
    "        continue\n",
    "        \n",
    "    # Save parameters for layers\n",
    "    if layer.get_weights():\n",
    "        # Get the weights and biases of the first layer\n",
    "        if hasattr(layer, 'bias') and layer.bias is not None:    # Check this line!!!\n",
    "            kernel, bias = layer.get_weights()\n",
    "            # Reshape parameters\n",
    "            kernel_arr = kernel.reshape((-1,kernel.shape[-1]))\n",
    "            bias_arr = bias\n",
    "            # apply quantization for all elements and convert to integer\n",
    "            vec_quantize = np.vectorize(quantize)    # function for array input\n",
    "            kernel_arr =  vec_quantize(kernel_arr)\n",
    "            kernel_arr =  kernel_arr.astype(\"int\"+\"{}\".format(N_QuantBits))\n",
    "            \n",
    "            bias_arr = vec_quantize(bias_arr)\n",
    "            bias_arr =  bias_arr.astype(\"int\"+\"{}\".format(N_QuantBits))\n",
    "            \n",
    "            # Convert to binary\n",
    "            vec_bin = np.vectorize(lambda x: np.binary_repr(x, width=N_QuantBits))\n",
    "            kernel_bin = vec_bin(kernel_arr)\n",
    "            bias_bin = vec_bin(bias_arr)\n",
    "            # Save to file\n",
    "            kernel_bin.tofile(file)\n",
    "            bias_bin.tofile(file)  \n",
    "        else:\n",
    "            kernel = layer.get_weights()\n",
    "            # Reshape parameters\n",
    "            kernel_arr = kernel.reshape((-1,kernel.shape[-1]))\n",
    "            # apply quantization for all elements and convert to integer\n",
    "            vec_quantize = np.vectorize(quantize)    # function for array input\n",
    "            kernel_arr =  vec_quantize(kernel_arr)\n",
    "            kernel_arr =  kernel_arr.astype(\"int\"+\"{}\".format(N_QuantBits))\n",
    "    \n",
    "            # Convert to binary\n",
    "            vec_bin = np.vectorize(lambda x: np.binary_repr(x, width=N_QuantBits))\n",
    "            kernel_bin = vec_bin(kernel_arr)\n",
    "           \n",
    "            # Save to file\n",
    "            kernel_bin.tofile(file)\n",
    "            \n",
    "    previous_layer = layer.name\n",
    "# Close the file\n",
    "file.close()\n",
    "print(\"Finish writing file !!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d328b63",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'binary_array1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Open a binary file in write mode\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary_file.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Write each binary array to file using the tofile() method\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mbinary_array1\u001b[49m\u001b[38;5;241m.\u001b[39mtofile(file)\n\u001b[1;32m      6\u001b[0m     binary_array2\u001b[38;5;241m.\u001b[39mtofile(file)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'binary_array1' is not defined"
     ]
    }
   ],
   "source": [
    "# Open a binary file in write mode\n",
    "with open(\"binary_file.bin\", \"wb\") as file:\n",
    "\n",
    "    # Write each binary array to file using the tofile() method\n",
    "    binary_array1.tofile(file)\n",
    "    binary_array2.tofile(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40edd987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 13:28:19.277433: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-09 13:28:19.489793: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-09 13:28:19.489821: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-09 13:28:20.572771: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-09 13:28:20.572873: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-09 13:28:20.572884: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-03-09 13:28:21.739337: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-09 13:28:21.739368: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-09 13:28:21.739396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (kun-Latitude-E5470): /proc/driver/nvidia/version does not exist\n",
      "2023-03-09 13:28:21.739620: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " quant_conv2d (quant_conv2D)    (None, 32, 32, 16)   448         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 16)  64          ['quant_conv2d[0][0]']           \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " quant_activation (quant_activa  (None, 32, 32, 16)  0           ['batch_normalization[0][0]']    \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 16)   0           ['quant_activation[0][0]']       \n",
      "                                                                                                  \n",
      " quant_conv2d_1 (quant_conv2D)  (None, 32, 32, 16)   2320        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 16)  64          ['quant_conv2d_1[0][0]']         \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " quant_activation_1 (quant_acti  (None, 32, 32, 16)  0           ['batch_normalization_1[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 16)   0           ['quant_activation_1[0][0]']     \n",
      "                                                                                                  \n",
      " quant_conv2d_2 (quant_conv2D)  (None, 32, 32, 16)   2320        ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 16)  64          ['quant_conv2d_2[0][0]']         \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 32, 32, 16)   0           ['activation[0][0]',             \n",
      "                                                                  'batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " quant_activation_2 (quant_acti  (None, 32, 32, 16)  0           ['add[0][0]']                    \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 32, 32, 16)   0           ['quant_activation_2[0][0]']     \n",
      "                                                                                                  \n",
      " quant_conv2d_3 (quant_conv2D)  (None, 16, 16, 32)   4640        ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 16, 16, 32)  128         ['quant_conv2d_3[0][0]']         \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " quant_activation_3 (quant_acti  (None, 16, 16, 32)  0           ['batch_normalization_3[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 16, 16, 32)   0           ['quant_activation_3[0][0]']     \n",
      "                                                                                                  \n",
      " quant_conv2d_4 (quant_conv2D)  (None, 16, 16, 32)   9248        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " quant_conv2d_5 (quant_conv2D)  (None, 16, 16, 32)   544         ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 16, 16, 32)  128         ['quant_conv2d_4[0][0]']         \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 16, 16, 32)   0           ['quant_conv2d_5[0][0]',         \n",
      "                                                                  'batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " quant_activation_4 (quant_acti  (None, 16, 16, 32)  0           ['add_1[0][0]']                  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 16, 16, 32)   0           ['quant_activation_4[0][0]']     \n",
      "                                                                                                  \n",
      " quant_conv2d_6 (quant_conv2D)  (None, 8, 8, 64)     18496       ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 8, 8, 64)    256         ['quant_conv2d_6[0][0]']         \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " quant_activation_5 (quant_acti  (None, 8, 8, 64)    0           ['batch_normalization_5[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 8, 8, 64)     0           ['quant_activation_5[0][0]']     \n",
      "                                                                                                  \n",
      " quant_conv2d_7 (quant_conv2D)  (None, 8, 8, 64)     36928       ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " quant_conv2d_8 (quant_conv2D)  (None, 8, 8, 64)     2112        ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 8, 8, 64)    256         ['quant_conv2d_7[0][0]']         \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 8, 8, 64)     0           ['quant_conv2d_8[0][0]',         \n",
      "                                                                  'batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " quant_activation_6 (quant_acti  (None, 8, 8, 64)    0           ['add_2[0][0]']                  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 8, 8, 64)     0           ['quant_activation_6[0][0]']     \n",
      "                                                                                                  \n",
      " quant_conv2d_9 (quant_conv2D)  (None, 4, 4, 128)    73856       ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 4, 4, 128)   512         ['quant_conv2d_9[0][0]']         \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " quant_activation_7 (quant_acti  (None, 4, 4, 128)   0           ['batch_normalization_7[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 4, 4, 128)    0           ['quant_activation_7[0][0]']     \n",
      "                                                                                                  \n",
      " quant_conv2d_10 (quant_conv2D)  (None, 4, 4, 128)   147584      ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " quant_conv2d_11 (quant_conv2D)  (None, 4, 4, 128)   8320        ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 4, 4, 128)   512         ['quant_conv2d_10[0][0]']        \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 4, 4, 128)    0           ['quant_conv2d_11[0][0]',        \n",
      "                                                                  'batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " quant_activation_8 (quant_acti  (None, 4, 4, 128)   0           ['add_3[0][0]']                  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 4, 4, 128)    0           ['quant_activation_8[0][0]']     \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 1, 1, 128)   0           ['activation_8[0][0]']           \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 128)          0           ['average_pooling2d[0][0]']      \n",
      "                                                                                                  \n",
      " quant_dense (quant_dense)      (None, 10)           1290        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 10)           0           ['quant_dense[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 310,090\n",
      "Trainable params: 309,098\n",
      "Non-trainable params: 992\n",
      "__________________________________________________________________________________________________\n",
      "(27, 16)\n",
      "kernel_bin shape:  (27, 16)\n",
      "(144, 16)\n",
      "kernel_bin shape:  (144, 16)\n",
      "(144, 16)\n",
      "kernel_bin shape:  (144, 16)\n",
      "(144, 32)\n",
      "kernel_bin shape:  (144, 32)\n",
      "(288, 32)\n",
      "(16, 32)\n",
      "(288, 64)\n",
      "kernel_bin shape:  (288, 64)\n",
      "(576, 64)\n",
      "(32, 64)\n",
      "(576, 128)\n",
      "kernel_bin shape:  (576, 128)\n",
      "(1152, 128)\n",
      "(64, 128)\n",
      "finish writing file !!!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import keras\n",
    "#from qkeras.utils import fold_batch_norm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras_model_quant_8bits import quant_conv2D,quant_activation, quant_dense\n",
    "custom_objects = {'quant_conv2D':  quant_conv2D, 'quant_activation':quant_activation, 'quant_dense':quant_dense}\n",
    "\n",
    "model = load_model('trained_models/trainedResnet_quant_8bits.h5', custom_objects = custom_objects)\n",
    "#model = fold_batch_norm(model)\n",
    "model.summary()\n",
    "\n",
    "file = open(\"weights.h\", \"w\")\n",
    "file.write(\"#This file contain parameter of model\\n\")\n",
    "\n",
    "N_QuantBits = 8;\n",
    "epsilon = 1e-6;\n",
    "\n",
    "# Quantization function\n",
    "def quantize(W):\n",
    "  # Forward computation\n",
    "  quantized_range = 2**(N_QuantBits-1)\n",
    "  f = tf.round(W*quantized_range)/quantized_range\n",
    "  f = int(quantized_range*(f))\n",
    "  return f\n",
    "\n",
    "def batch_fold(mu,var,gamma, beta, epsilon, weights, biases):\n",
    "    \n",
    "    param1 = gamma*math.sqrt(var*var+epsilon)\n",
    "    param2 = -param1*mu + beta\n",
    "    \n",
    "    weights_new = weights*param1\n",
    "    biases_new = biases*param1 + param2\n",
    "    return weights_new,biases_new\n",
    "\n",
    "class my_conv:\n",
    "    def __init__(self, var1, var2):\n",
    "        self.w = var1\n",
    "        self.b = var2\n",
    "    def set_params(self, var1, var2):\n",
    "        self.w = var1\n",
    "        self.b = var2\n",
    "    def batch_fold(self,mu,var,gamma, beta, epsilon):\n",
    "    \n",
    "        param1 = gamma*np.sqrt(var*var+epsilon)\n",
    "        param2 = -param1*mu + beta\n",
    "        p = self.w\n",
    "        weights_new = self.w*param1\n",
    "        biases_new = self.b*param1 + param2\n",
    "        self.set_params(weights_new,biases_new)\n",
    "        #return weights_new,biases_new\n",
    "    def delete_params(self):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "quan_conv1 = my_conv(None, None)   # Using for batch_normalization folding\n",
    "quan_conv2 = my_conv(None, None)\n",
    "previous_layer = None\n",
    "#print(quan_conv1.w, quan_conv1.b)\n",
    "num_quant_layer = 0;\n",
    "for layer in model.layers:   \n",
    "    # Save parameters for batch normalization\n",
    "    if isinstance(layer, keras.layers.BatchNormalization):\n",
    "        \n",
    "       # Get the gamma, beta, moving_mean, and moving_variance parameters of the layer\n",
    "        gamma, beta, moving_mean, moving_variance = layer.get_weights()\n",
    "        #print(\"batch norm\",gamma.shape)\n",
    "       # Batch folding\n",
    "        quan_conv1.batch_fold(moving_mean,moving_variance,gamma, beta,epsilon)\n",
    "        \n",
    "        if (quan_conv2.w is None) and (quan_conv2.b is None): \n",
    "            # apply quantization for all elements and convert to integer\n",
    "            vec_quantize = np.vectorize(quantize)    # function for array input\n",
    "            kernel_arr =  vec_quantize(quan_conv1.w)\n",
    "            kernel_arr =  kernel_arr.astype(\"int\"+\"{}\".format(N_QuantBits))\n",
    "            \n",
    "            bias_arr = vec_quantize(quan_conv1.b)\n",
    "            bias_arr =  bias_arr.astype(\"int\"+\"{}\".format(N_QuantBits))\n",
    "            \n",
    "            # Convert to binary\n",
    "            vec_bin = np.vectorize(lambda x: np.binary_repr(x, width=N_QuantBits))\n",
    "            kernel_bin = vec_bin(kernel_arr)\n",
    "            print(\"kernel_bin shape: \",kernel_bin.shape)\n",
    "            bias_bin = vec_bin(bias_arr)\n",
    "            # Save to file\n",
    "            file.write(\"quan_conv\" +\"{}\".format(num_quant_layer)+\"_w\" + \"[{}]\".format(np.prod(kernel_bin.shape)) + \"= {\\n\")\n",
    "            for element in kernel_bin:\n",
    "                file.write(str(element) + ',\\n')\n",
    "            file.write(str(element) + '}\\n')\n",
    "            \n",
    "            file.write(\"quan_conv\" +\"{}\".format(num_quant_layer) + \"_b\" + \"[{}]\".format(np.prod(bias_bin.shape)) + \"= {\\n\")\n",
    "            for element in bias_bin: \n",
    "                file.write(str(element) + '\\n')\n",
    "            file.write(str(element) + '}\\n')\n",
    "                \n",
    "            #kernel_bin.tofile(file)\n",
    "            #bias_bin.tofile(file)\n",
    "        else:\n",
    "            # Save simultaneous 2 quant_conv to binary file\n",
    "            \n",
    "            # The first one\n",
    "            # apply quantization for all elements and convert to integer\n",
    "            vec_quantize = np.vectorize(quantize)    # function for array input\n",
    "            kernel_arr =  vec_quantize(quan_conv1.w)\n",
    "            kernel_arr =  kernel_arr.astype(\"int\"+\"{}\".format(N_QuantBits))\n",
    "            \n",
    "            bias_arr = vec_quantize(quan_conv1.b)\n",
    "            bias_arr =  bias_arr.astype(\"int\"+\"{}\".format(N_QuantBits))\n",
    "            \n",
    "            # Convert to binary\n",
    "            vec_bin = np.vectorize(lambda x: np.binary_repr(x, width=N_QuantBits))\n",
    "            kernel_bin = vec_bin(kernel_arr)\n",
    "            bias_bin = vec_bin(bias_arr)\n",
    "            # Save to file\n",
    "            file.write(\"quan_conv\" +\"{}\".format(num_quant_layer-1)+\"_w\" + \"[{}]\".format(np.prod(kernel_bin.shape)) + \"= {\\n\")\n",
    "            for element in kernel_bin:\n",
    "                file.write(str(element) + ',\\n')\n",
    "            file.write(str(element) + '}\\n')\n",
    "            \n",
    "            file.write(\"quan_conv\" +\"{}\".format(num_quant_layer-1) + \"_b\" + \"[{}]\".format(np.prod(bias_bin.shape)) + \"= {\\n\")\n",
    "            for element in bias_bin: \n",
    "                file.write(str(element) + '\\n')\n",
    "            file.write(str(element) + '}\\n')\n",
    "            \n",
    "            # The second one\n",
    "            # apply quantization for all elements and convert to integer\n",
    "            vec_quantize = np.vectorize(quantize)    # function for array input\n",
    "            kernel_arr =  vec_quantize(quan_conv2.w)\n",
    "            kernel_arr =  kernel_arr.astype(\"int\"+\"{}\".format(N_QuantBits))\n",
    "            \n",
    "            bias_arr = vec_quantize(quan_conv2.b)\n",
    "            bias_arr =  bias_arr.astype(\"int\"+\"{}\".format(N_QuantBits))\n",
    "            \n",
    "            # Convert to binary\n",
    "            vec_bin = np.vectorize(lambda x: np.binary_repr(x, width=N_QuantBits))\n",
    "            kernel_bin = vec_bin(kernel_arr)\n",
    "            bias_bin = vec_bin(bias_arr)\n",
    "            # Save to file\n",
    "            file.write(\"quan_conv\" +\"{}\".format(num_quant_layer)+\"_w\" + \"[{}]\".format(np.prod(kernel_bin.shape)) + \"= {\\n\")\n",
    "            for element in kernel_bin:\n",
    "                file.write(str(element) + ',\\n')\n",
    "            file.write(str(element) + '}\\n')\n",
    "            \n",
    "            file.write(\"quan_conv\" +\"{}\".format(num_quant_layer) + \"_b\" + \"[{}]\".format(np.prod(bias_bin.shape)) + \"= {\\n\")\n",
    "            for element in bias_bin: \n",
    "                file.write(str(element) + '\\n')\n",
    "            file.write(str(element) + '}\\n')\n",
    "        # Delete parameters of 2 object quan_conv1 and quan_conv2 (for saving new parameters)\n",
    "        quan_conv1.delete_params()\n",
    "        quan_conv2.delete_params()\n",
    "        continue\n",
    "    #Save parameters for quant_conv2D\n",
    "    if 'quant_conv2d' in layer.name:    \n",
    "        num_quant_layer = num_quant_layer+1\n",
    "        kernel, bias = layer.get_weights() \n",
    "        # reshape kernel and bias to (,num_numfilters)\n",
    "        kernel_arr = kernel.reshape((-1,kernel.shape[-1]))\n",
    "        bias_arr = bias\n",
    "        print(kernel_arr.shape)\n",
    "        \n",
    "        # Check if there is exist 2 continuous layer\n",
    "        if quan_conv1.w is None and quan_conv1.b is None:\n",
    "            quan_conv1.set_params(kernel_arr,bias_arr)\n",
    "        else:\n",
    "            quan_conv2.set_params(kernel_arr,bias_arr)\n",
    "        continue\n",
    "        \n",
    "    # Save parameters for other layers\n",
    "    if layer.get_weights():\n",
    "        # Get the weights and biases of the first layer\n",
    "        if hasattr(layer, 'bias') and layer.bias is not None:    # Check this line!!!\n",
    "            kernel, bias = layer.get_weights()\n",
    "            # Reshape parameters\n",
    "            kernel_arr = kernel.reshape((-1,kernel.shape[-1]))\n",
    "            bias_arr = bias\n",
    "            # apply quantization for all elements and convert to integer\n",
    "            vec_quantize = np.vectorize(quantize)    # function for array input\n",
    "            kernel_arr =  vec_quantize(kernel_arr)\n",
    "            kernel_arr =  kernel_arr.astype(\"int\"+\"{}\".format(N_QuantBits))\n",
    "            \n",
    "            bias_arr = vec_quantize(bias_arr)\n",
    "            bias_arr =  bias_arr.astype(\"int\"+\"{}\".format(N_QuantBits))\n",
    "            \n",
    "            # Convert to binary\n",
    "            vec_bin = np.vectorize(lambda x: np.binary_repr(x, width=N_QuantBits))\n",
    "            kernel_bin = vec_bin(kernel_arr)\n",
    "            bias_bin = vec_bin(bias_arr)\n",
    "            # Save to file\n",
    "            # Save to file\n",
    "            file.write(\"dense_w\" + \"[{}]\".format(np.prod(kernel_bin.shape)) + \"= {\\n\")\n",
    "            for element in kernel_bin:\n",
    "                file.write(str(element) + ',\\n')\n",
    "            file.write(str(element) + '}\\n')\n",
    "            \n",
    "            file.write(\"dense_b\" +\"[{}]\".format(np.prod(bias_bin.shape)) + \"= {\\n\")\n",
    "            for element in bias_bin: \n",
    "                file.write(str(element) + '\\n')\n",
    "            file.write(str(element) + '}\\n') \n",
    "        else:\n",
    "            kernel = layer.get_weights()\n",
    "            # Reshape parameters\n",
    "            kernel_arr = kernel.reshape((-1,kernel.shape[-1]))\n",
    "            # apply quantization for all elements and convert to integer\n",
    "            vec_quantize = np.vectorize(quantize)    # function for array input\n",
    "            kernel_arr =  vec_quantize(kernel_arr)\n",
    "            kernel_arr =  kernel_arr.astype(\"int\"+\"{}\".format(N_QuantBits))\n",
    "    \n",
    "            # Convert to binary\n",
    "            vec_bin = np.vectorize(lambda x: np.binary_repr(x, width=N_QuantBits))\n",
    "            kernel_bin = vec_bin(kernel_arr)\n",
    "           \n",
    "            # Save to file\n",
    "            file.write(\"dense_w\" + \"[{}]\".format(np.prod(kernel_bin.shape)) + \"= {\\n\")\n",
    "            for element in kernel_bin:\n",
    "                file.write(str(element) + ',\\n')\n",
    "            file.write(str(element) + '}\\n')\n",
    "            \n",
    "    previous_layer = layer.name\n",
    "# Close the file\n",
    "file.close()\n",
    "print(\"finish writing file !!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9477d8d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
